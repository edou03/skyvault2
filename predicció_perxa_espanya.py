# -*- coding: utf-8 -*-
"""Predicci√≥_perxa_Espanya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7dzMBUfwSBf37xF6jhgd5Hzy_2FoVAp
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

informes =pd.read_excel('/content/drive/MyDrive/TFG/Biomecanica CAR/Informes/informes_espanÃÉa.xlsx')
informes.info()

# prompt: la variable Talla quiero pasarla a numerica

import pandas as pd
# Convert 'Talla' column to numeric, handling errors
informes['Talla'] = pd.to_numeric(informes['Talla'], errors='coerce')

# Check the data type of the 'Talla' column
print(informes['Talla'].dtype)

# Print some info
informes.info()

# Imputar valors faltants de 'Talla' segons la mediana per g√®nere
informes['Talla'] = informes.groupby('Genero')['Talla'].transform(
    lambda x: x.fillna(x.median())
)

# Comprovar que ja no hi ha valors nuls a 'Talla'
print(informes['Talla'].isnull().sum())

# Mostrar info del DataFrame
informes.info()

# prompt: Dentro de la variable Genero, quiero pasarla a numerica. M = 1, F=0

# Replace 'M' with 1 and 'F' with 0 in the 'Genero' column
informes['Genero'] = informes['Genero'].replace({'M': 1, 'F': 0})

#Check the data type of the 'Genero' column
print(informes['Genero'].dtype)

# Print some info
informes.info()

"""1. ANALISIS DE VALORS NULS PER COLUMNA"""

# Calcular el percentatge de valors nuls per columna
null_percent =  informes.isnull().mean().sort_values(ascending=False) * 100
null_percent = null_percent[null_percent > 0]  # Mostrem nom√©s les que tenen valors nuls
null_percent.round(2)

informes.dtypes.value_counts()

"""2. VARIABLES PREDICTIVAS

* < 5% de valores nulos ‚Üí Imputaci√≥n sin mayores problemas.
* 5% ‚Äì 30% ‚Üí Se considera imputar si la variable es relevante, aunque se eval√∫a con m√°s cuidado.
* (> 30%) ‚Üí Generalmente se descarta, salvo que haya una justificaci√≥n s√≥lida para imputar (por ejemplo, alto poder predictivo o imputaci√≥n fiable basada en otras variables).
"""

# Eliminarem les variebles que no ens aporten res a l'estudi.
# Comencem per les que no s√≥n predictives.

# prompt: Quiero quitar las siguientes variables: nombre_atleta, ID_salto, fecha, Carrera, pole lbs

# Drop the specified columns
informes = informes.drop(columns=['nombre_atleta', 'ID_salto','fecha', 'Carrera', 'pole lbs'], errors='ignore')

# Print some info
informes.info()

# Vel. horizontal clavada t√© 66,87% de valors nuls, no el volem a l'estudi.
informes = informes.drop(columns=['Vel. horizontal clavada'], errors='ignore')
informes.info()

# prompt: tamb√© vull eliminar:
# vel. horizontal ultimo, vel.vertical take_off, longitud antepenultimo, longitud antepenultimo relativo, angulo salida

# Continue dropping columns
informes = informes.drop(columns=['Vel. horizontal ultimo','Vel. horizontal Take off', 'Vel. vertical Take Off', 'Longitud antepenultimo', 'Longitud antepenultimo relativo', 'Angulo salida'], errors='ignore')

# Print some info
informes.info()

"""**IMPUTEM**: Pole flex, Distancia horizontal impulso/agarre, Pasos, Angulo Pertiga, grip width"""

import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

df = informes.copy()

# Codificar 'Genero' si √©s tipus objecte
if df['Genero'].dtype == 'object':
    df['Genero'] = df['Genero'].astype('category').cat.codes

# üîπ Eliminar columnes no desitjades
df = df.drop(columns=[
    'Pasos',
    'Longitud relativa penultimo paso',
    'Longitud penultimo paso',
    'Longitud ultimo relativo al penultimo'
])

# üîπ Imputaci√≥ de 'peso' amb regressi√≥ (usant 'Genero' i 'Talla')
df_peso = df[['Genero', 'Talla', 'peso']].dropna()
X_peso = df_peso[['Genero', 'Talla']]
y_peso = df_peso['peso']

model_peso = LinearRegression().fit(X_peso, y_peso)
missing_peso = df['peso'].isna()
X_missing_peso = df.loc[missing_peso, ['Genero', 'Talla']]
df.loc[missing_peso, 'peso'] = model_peso.predict(X_missing_peso)

# üîπ Nova imputaci√≥ de 'pole flex' amb Ridge i variables millorades
features_flex = [
    'pole length', 'Altura agarre en batida', 'Genero', 'peso', 'Talla',
    'Velocidad 10-5m', 'Aceleracion 10-5m',
    'Distancia horizontal impulso/agarre', 'grip', 'Grip width'
]

df_flex_ridge = df[features_flex + ['pole flex']].dropna()
X_flex = df_flex_ridge[features_flex]
y_flex = df_flex_ridge['pole flex']

model_ridge = make_pipeline(StandardScaler(), Ridge(alpha=1.0))
model_ridge.fit(X_flex, y_flex)

missing_flex = df['pole flex'].isna()
X_missing_flex = df.loc[missing_flex, features_flex].fillna(df[features_flex].mean())
df.loc[missing_flex, 'pole flex'] = model_ridge.predict(X_missing_flex)

# üîπ Imputaci√≥ de 'Angulo pertiga' i 'Grip width' per grups
cols_group_mean = ['Angulo pertiga', 'Grip width']
for col in cols_group_mean:
    if df[col].isna().sum() > 0:
        df[col] = df.groupby(['Genero', 'ID_atleta'])[col].transform(lambda x: x.fillna(x.mean()))
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna(df[col].mean())

# üîπ Imputaci√≥ de 'Distancia horizontal impulso/agarre'
if df['Distancia horizontal impulso/agarre'].isna().sum() > 0:
    df['Distancia horizontal impulso/agarre'] = df.groupby(['Genero', 'ID_atleta'])['Distancia horizontal impulso/agarre'].transform(lambda x: x.fillna(x.mean()))
    df['Distancia horizontal impulso/agarre'] = df['Distancia horizontal impulso/agarre'].fillna(df['Distancia horizontal impulso/agarre'].mean())

# üîç Informaci√≥ final del dataframe
df.info()

df['Longitud relativa ultimo paso'].fillna(df['Longitud relativa ultimo paso'].median(), inplace=True)
df['Altura relativa agare en batida'].fillna(df['Altura relativa agare en batida'].median(), inplace=True)
df.info()

"""Ara he de decidir quines variables imputo i en funci√≥ de qu√® i quines variables elimino (peso?)"""

#Vull eliminar les seg√ºents variables perqu√® percentatge elevat de Nan:

# Check existing columns before dropping
columns_to_drop = [
    'Pasos',
    'Longitud relativa penultimo paso',
    'Longitud penultimo paso',
    'Longitud ultimo relativo al penultimo'
]

# Only drop columns that actually exist in the DataFrame
existing_columns = [col for col in columns_to_drop if col in df.columns]

df = df.drop(columns=existing_columns)
df.info()

"""# VARIABLES PREDICTIVES"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# -------------------------------
# 1. NOVA SELECCI√ì DE VARIABLES
# -------------------------------
feature_cols = [
    'Genero', 'Talla', 'peso',
    'Velocidad 10-5m', 'Aceleracion 10-5m',
    'Distancia Batida', 'Altura agarre en batida',
    'Distancia horizontal impulso/agarre', 'Distancia horizontal impulso/agarre 2',
    'grip', 'Angulo pertiga',
    'pole length', 'pole flex'
]

X = df[feature_cols]
y = df['height bar']

# -------------------------------
# 2. DIVISI√ì DELS DADES
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------------------
# 3. PIPELINE: LINEAR REGRESSION
# -------------------------------
pipeline_lr = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('model', LinearRegression())
])

pipeline_lr.fit(X_train, y_train)
y_pred_lr = pipeline_lr.predict(X_test)

mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print("üîç Linear Regression amb Pipeline")
print(f"Mean Squared Error: {mse_lr:.4f}")
print(f"R-squared: {r2_lr:.4f}")

# -------------------------------
# 4. PIPELINE: XGBoost
# -------------------------------
pipeline_xgb = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('model', xgb.XGBRegressor(random_state=42))
])

pipeline_xgb.fit(X_train, y_train)
y_pred_xgb = pipeline_xgb.predict(X_test)

mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print("\nüå≤ XGBoost amb Pipeline")
print(f"Mean Squared Error: {mse_xgb:.4f}")
print(f"R-squared: {r2_xgb:.4f}")

# -------------------------------
# 5. NOVA FUNCI√ì DE PREDICCI√ì
# -------------------------------
def predict_height(model_pipeline, input_dict):
    """
    Prediu la height bar a partir de les caracter√≠stiques d'entrada (diccionari).
    """
    input_df = pd.DataFrame([input_dict])
    return model_pipeline.predict(input_df)[0]

# Exemple de predicci√≥ (has d'omplir els valors reals)
sample_input = {
    'Genero': 1,
    'Talla': 1.84,
    'peso': 78,
    'Velocidad 10-5m': 10.3,
    'Aceleracion 10-5m': 0.3,
    'Distancia Batida': 4.2,
    'Altura agarre en batida': 2.2,
    'Distancia horizontal impulso/agarre': 0.44,
    'Distancia horizontal impulso/agarre 2': 0.26,
    'grip': 5.15,
    'Angulo pertiga': 30,
    'pole length': 5.25,
    'pole flex': 10
}

altura_pred_lr = predict_height(pipeline_lr, sample_input)
altura_pred_xgb = predict_height(pipeline_xgb, sample_input)

print(f"\nüéØ Predicci√≥ amb Linear Regression: {altura_pred_lr:.3f} m")
print(f"üéØ Predicci√≥ amb XGBoost: {altura_pred_xgb:.3f} m")

import matplotlib.pyplot as plt

def plot_predictions(y_true, y_pred, title=''):
    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred, alpha=0.6, edgecolor='k')
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--r')
    plt.xlabel('Valor real (Height bar)')
    plt.ylabel('Predicci√≥')
    plt.title(title)
    plt.grid(True)
    plt.show()

plot_predictions(y_test, y_pred_lr, title='Linear Regression - Prediccions vs Reals')
plot_predictions(y_test, y_pred_xgb, title='XGBoost - Prediccions vs Reals')

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validation amb Linear Regression
cv_scores_lr = cross_val_score(pipeline_lr, X, y, cv=cv, scoring='r2')
print("üîÅ Linear Regression - R¬≤ mitj√† (CV):", np.mean(cv_scores_lr))

# Cross-validation amb XGBoost
cv_scores_xgb = cross_val_score(pipeline_xgb, X, y, cv=cv, scoring='r2')
print("üîÅ XGBoost - R¬≤ mitj√† (CV):", np.mean(cv_scores_xgb))

import matplotlib.pyplot as plt

# --- Taula de coeficients ---
# Access the Linear Regression model from the pipeline using 'named_steps'
model = pipeline_lr.named_steps['model']

coeficients = pd.DataFrame({
    'Variable': X.columns,
    'Pes (coeficient)': model.coef_
})
coeficients['Pes absolut'] = coeficients['Pes (coeficient)'].abs()
coeficients = coeficients.sort_values(by='Pes absolut', ascending=False)

# % de pes relatiu
total_pes = coeficients['Pes absolut'].sum()
coeficients['% Pes relatiu'] = 100 * coeficients['Pes absolut'] / total_pes

# Mostrar taula
print(coeficients)

# --- Gr√†fic ---
plt.figure(figsize=(10, 6))
plt.barh(coeficients['Variable'], coeficients['% Pes relatiu'], color='skyblue')
plt.xlabel('% Pes relatiu')
plt.title('üìä Influ√®ncia de cada variable (Linear Regression)')
plt.gca().invert_yaxis()  # La m√©s important a dalt
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Import√†ncia de variables amb XGBoost
# -------------------------------
importances = pipeline_xgb.named_steps['model'].feature_importances_

# Get the feature names used by the pipeline
# üêõ Fix: Use the stored feature_names_in_ instead of feature_cols
feature_names = pipeline_xgb.named_steps['imputer'].feature_names_in_

importancia_xgb = pd.DataFrame({
    'Variable': feature_names,  # Use feature_names instead of X.columns
    'Import√†ncia': importances
}).sort_values(by='Import√†ncia', ascending=False)

# Mostrar taula
print(importancia_xgb)

# Gr√†fic
plt.figure(figsize=(10, 6))
plt.barh(importancia_xgb['Variable'], importancia_xgb['Import√†ncia'], color='salmon')
plt.xlabel('Import√†ncia (feature importance)')
plt.title('üå≤ Import√†ncia de variables (XGBoost)')
plt.gca().invert_yaxis()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Calcular MSE i MAE
mse_lr = mean_squared_error(y_test, y_pred_lr)
mae_lr = np.mean(np.abs(y_test - y_pred_lr))

# Convertir a cent√≠metres
mse_cm = mse_lr * 100**2
mae_cm = mae_lr * 100

print("üîç Linear Regression:")
print(f"Mean Squared Error (m): {mse_lr:.4f}")
print(f"Mean Squared Error (cm¬≤): {mse_cm:.2f}")
print(f"Mean Absolute Error (cm): {mae_cm:.2f}")

# Calcular MSE i MAE per a XGBoost
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
mae_xgb = np.mean(np.abs(y_test - y_pred_xgb))

# Convertir a cent√≠metres
mse_xgb_cm = mse_xgb * 100**2
mae_xgb_cm = mae_xgb * 100

print("üå≤ XGBoost Regressor:")
print(f"Mean Squared Error (m): {mse_xgb:.4f}")
print(f"Mean Squared Error (cm¬≤): {mse_xgb_cm:.2f}")
print(f"Mean Absolute Error (cm): {mae_xgb_cm:.2f}")

# Gr√†fic circular del pes relatiu
plt.figure(figsize=(8, 8))
plt.pie(coeficients['% Pes relatiu'], labels=coeficients['Variable'],
        autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20.colors)
plt.title('ü•ß Distribuci√≥ del pes relatiu de les variables (Linear Regression)')
plt.axis('equal')  # Assegura forma de cercle
plt.tight_layout()
plt.show()

# üîÆ CEL¬∑LA FINAL PER A LA PREDICCI√ì MANUAL

# Introdueix aqu√≠ les teves dades reals:
dades_reals = {
    'Genero': 1,  # 0 = Dona, 1 = Home
    'Talla': 1.8,
    'peso': 68,
    'Velocidad 10-5m': 9,
    'Aceleracion 10-5m': 0.3,
    'Distancia Batida': 3.5,
    'Altura agarre en batida': 2.1,
    'Distancia horizontal impulso/agarre': 0.2,
    'Distancia horizontal impulso/agarre 2': 0.1,
    'grip': 4.4,
    'Angulo pertiga': 28,
    'pole length': 4.9,
    'pole flex': 16.8
}

# Predicci√≥ amb els dos models
pred_lr = predict_height(pipeline_lr, dades_reals)
pred_xgb = predict_height(pipeline_xgb, dades_reals)

print(f"\nüéØ Predicci√≥ amb Linear Regression: {pred_lr:.3f} m")
print(f"üéØ Predicci√≥ amb XGBoost: {pred_xgb:.3f} m")

"""# **RESUM:**
### 1.   Objectius:

*   Predir l'al√ßada del salt amb perxa (height_bar)
*   Analitzar quines variables influeixen m√©s en el rendiment
  
### 2. Dades disponibles:
*   138 mostres (49 atletes)
*   39 variables a analitzar
*   Variable objectiu: height_bar

### 3. Selecci√≥ de Variables Predictives.
*   13 variables predictives en base a:
    *   Rellev√†ncia te√≤rica

    * Qualitat de les dades

    * Percentatge de valors informats

### 4. Modelitzaci√≥:
* Regressi√≥ Lineal

* XGBoost Regressor

* Entrenats mitjan√ßant train_test_split (80/20)

# OBSERVACIONS:

*   Valorar quines variables s√≥n rellevants i quines no

*   Fer una altra predicci√≥ amb menys variables ( informaci√≥ in-situ)

*   Potser s'ha de trovar la manera de que si no introdueixes una variable no   dongui problemas, si no que no la contempli

*   Qu√® passa amb la variable Genero?

# APLICACI√ìN
"""

import pickle

# NOVES COLUMNES SELECCIONADES
feature_cols = [
    'Genero', 'Talla', 'peso',
    'Velocidad 10-5m', 'Aceleracion 10-5m',
    'Distancia Batida','grip', 'pole length', 'pole flex'
]

X = df[feature_cols]
y = df['height bar']

# ENTRENAMENT
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

pipeline_lr = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('model', LinearRegression())
])

pipeline_lr.fit(X, y)

# GUARDA EL MODEL
with open("model_lr.pkl", "wb") as f:
    pickle.dump(pipeline_lr, f)

# DESCARREGA EL FITXER
from google.colab import files
files.download("model_lr.pkl")

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

import streamlit as st
import pickle
import numpy as np
import pandas as pd

# Carrega el model
with open("model_lr.pkl", "rb") as f:
    model = pickle.load(f)

st.title("üéØ Predicci√≥ de l'al√ßada en el salt amb perxa")

st.markdown("Introdueix les teves dades per predir l'al√ßada estimada del teu salt:")

# Inputs
genero = st.selectbox("G√®nere", ["Home", "Dona"])
talla = st.number_input("Talla (m)", min_value=1.0, max_value=2.5, step=0.01)
peso = st.number_input("Pes (kg)", min_value=30.0, max_value=120.0, step=0.1)
velocitat = st.number_input("Velocitat 10-5m (m/s)", min_value=0.0, step=0.01)
aceleracio = st.number_input("Aceleraci√≥ 10-5m (m/s¬≤)", min_value=0.0, step=0.01)
dist_batuda = st.number_input("Dist√†ncia de batuda (m)", min_value=0.0, step=0.01)
long_perxa = st.number_input("Longitud de la perxa (m)", min_value=3.0, max_value=6.5, step=0.01)
flexio_perxa = st.number_input("Flexi√≥ de la perxa", min_value=0.0, step=0.1)

# Converteix g√®nere a n√∫mero
genero_num = 1 if genero == "Home" else 0

# Predicci√≥
if st.button("Predir al√ßada"):
    input_data = pd.DataFrame([[
        genero_num, talla, peso, velocitat,
        aceleracio, dist_batuda, long_perxa, flexio_perxa
    ]], columns=[
        'Genero', 'Talla', 'peso',
        'Velocidad 10-5m', 'Aceleracion 10-5m',
        'Distancia Batida', 'pole length', 'pole flex'
    ])

    altura = model.predict(input_data)[0]
    st.success(f"üèÜ Al√ßada estimada: {altura:.2f} m")

import pickle
import pandas as pd

# üì• Carrega el model guardat
with open("model_lr.pkl", "rb") as f:
    model = pickle.load(f)

# üß™ Exemple de dades d‚Äôentrada (modifica-ho per provar diferents atletes)
input_data = {
    'Genero': 1,  # 1 = Home, 0 = Dona
    'Talla': 1.78,
    'peso': 68,
    'Velocidad 10-5m': 8.3,
    'Aceleracion 10-5m': 0.28,
    'Distancia Batida': 3.5,
    'grip': 4.8,
    'pole length': 4.9,
    'pole flex': 17
}

# üßÆ Converteix a DataFrame i prediu
input_df = pd.DataFrame([input_data])
prediccio = model.predict(input_df)[0]

# üì¢ Resultat
print(f"üéØ Al√ßada estimada del salt: {prediccio:.2f} m")

import pickle
import matplotlib.pyplot as plt
import pandas as pd

# üì• Carrega el model guardat
with open("model_lr.pkl", "rb") as f:
    model = pickle.load(f)

# üìà Extreu els noms de les variables i els coeficients
feature_names = ['Genero', 'Talla', 'peso', 'Velocidad 10-5m', 'Aceleracion 10-5m',
                 'Distancia Batida','grip', 'pole length', 'pole flex']

# Accedeix als coeficients dins del pipeline
coefficients = model.named_steps['model'].coef_

# üìä Crea un DataFrame per ordenar i visualitzar millor
coef_df = pd.DataFrame({
    'Variable': feature_names,
    'Pes (coeficient)': coefficients
}).sort_values(by='Pes (coeficient)', key=abs, ascending=False)

# üé® Ploteja
plt.figure(figsize=(10, 6))
plt.barh(coef_df['Variable'], coef_df['Pes (coeficient)'], color='teal')
plt.xlabel("Pes del coeficient (impacte sobre l'al√ßada)")
plt.title("üéØ Import√†ncia de les variables en el model de regressi√≥ lineal")
plt.gca().invert_yaxis()  # la m√©s important a dalt
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()